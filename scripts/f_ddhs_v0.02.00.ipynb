{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.datasets.demo import download_demo\n",
    "real_data, metadata = download_demo(\n",
    "    modality='single_table',\n",
    "    dataset_name='fake_hotel_guests')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "synthesizer.fit(data=real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sdv.lite import SingleTablePreset\n",
    "\n",
    "# push + prd\n",
    "\n",
    "# imbalanced에 data level로 해결하는 모델\n",
    "class FiGen:\n",
    "    def __init__(self, ratio: float, index: List[str]):\n",
    "        \"\"\"\n",
    "        고정적으로 사용하는 값을 저장\n",
    "        \n",
    "        Args:\n",
    "            ratio (float): small class+생성된 데이터와 large class의 비율 \n",
    "            index (List[int]): 범주형, 연속형 구분하기 위한 연속형 변수의 컬럼명 인덱스       \n",
    "        \"\"\"\n",
    "        self.result = 0\n",
    "        self.ratio = ratio\n",
    "        self.index = index\n",
    "\n",
    "\n",
    "    def extract_middle_percent(self, data: pd.DataFrame, start: float, last:float):\n",
    "        \"\"\"\n",
    "        데이터의 분포 중 중간 부분을 추출 \n",
    "        \n",
    "        Args:\n",
    "            data : 입력 데이터\n",
    "            start : 추출 시작 percentile \n",
    "            last : 추출 끝 percentile\n",
    "        Returns:    \n",
    "            데이터의 분포 중 중간 부분을 추출하여 리턴\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "        kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5).fit(\n",
    "            data_scaled\n",
    "        )  ##TODO: 계산이 안터지도록 하기, gmm으로 변경\n",
    "\n",
    "        log_prob = kde.score_samples(data_scaled)\n",
    "        prob = np.exp(log_prob)\n",
    "        threshold_low, threshold_high = np.percentile(prob, [start, last])\n",
    "        mask = np.logical_and(prob >= threshold_low, prob <= threshold_high)\n",
    "        data_middle = data[mask]\n",
    "\n",
    "        if len(data_middle) > 0:\n",
    "            return data_middle\n",
    "        else:\n",
    "            print(\"No middle 50% found, returning original data\")\n",
    "            return np.array([])\n",
    "\n",
    "    def find_categorical(\n",
    "        self, suitable_generated_small_X: pd.DataFrame, categorical_small_X: pd.DataFrame, small_X: pd.DataFrame\n",
    "    ):  # ****************\n",
    "        \"\"\"\n",
    "        생성된 연속형변수와 기존 연속형 변수의 cosine simmilarity를 기준으로 가장 가까운 기존 변수를 찾은 후 해당 변수의 범주형 값을 가져옴\n",
    "        \n",
    "        Args:\n",
    "            suitable_generated_small_X : 생성된 적합한 small class의 연속형 변수만 있는 x \n",
    "            small_X : small class의 연속형, 범주형 변수가 모두 있는 orgin x\n",
    "        Returns:\n",
    "            생성된 연속 변수를 범주형 변수값이 결합된 형태로 리턴 \n",
    "        \"\"\"\n",
    "\n",
    "        # Min-Max 스케일링을 위한 객체 생성\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # 열별 Min-Max 스케일링 수행\n",
    "        suitable_generated_small_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(suitable_generated_small_X),\n",
    "            columns=suitable_generated_small_X.columns,\n",
    "        )\n",
    "\n",
    "        orgin_small_non_cat_scaled_X = pd.DataFrame(\n",
    "            scaler.fit_transform(small_X[ self.index]),\n",
    "            columns=self.index\n",
    "        )\n",
    "\n",
    "        # 데이터프레임을 numpy 배열로 변환\n",
    "        array_mxn = suitable_generated_small_scaled_X.values\n",
    "        array_kxn = orgin_small_non_cat_scaled_X.values\n",
    "\n",
    "        # 행렬곱 수행 (mxn과 nxk로 계산)\n",
    "        result_array = np.dot(array_mxn, array_kxn)\n",
    "\n",
    "        # 각 행에서 최대값을 가지는 열의 인덱스를 가져와서 리스트로 만들기\n",
    "        max_indices = np.argmax(result_array, axis=1).tolist()\n",
    "\n",
    "        # 가장큰 열 인덱스가 들어있는 리스트의 인덱스에 따라 범주형 값 가져오기\n",
    "        synthetic_small_X = pd.concat(\n",
    "            [suitable_generated_small_scaled_X, categorical_small_X.loc[max_indices]],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return synthetic_small_X\n",
    "\n",
    "    def suitable_judge(self, midlle_small_X:pd.DataFrame, small_X: pd.DataFrame, large_X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "           generated_x : 생성된 small class x 데이터\n",
    "           small_X : 원본 small class x 데이터\n",
    "           large_X : 원본 large class x 데이터\n",
    "        \"\"\"\n",
    "\n",
    "        center_small_X = np.mean(\n",
    "            small_X.cpu().numpy(), axis=1, dtype=np.float64, out=None \n",
    "        )\n",
    "\n",
    "        radius_small_X = np.max(\n",
    "            np.linalg.norm(small_X.cpu().numpy() - center_small_X, axis=1)\n",
    "        )\n",
    "\n",
    "        center_large_X = np.mean(\n",
    "            large_X.cpu().numpy(), axis=1, dtype=np.float64, out=None \n",
    "        )\n",
    "\n",
    "        radius_large_X = np.max(\n",
    "            np.linalg.norm(large_X.cpu().numpy() - center_large_X, axis=1)\n",
    "        )\n",
    "\n",
    "        synthetic_sample = pd.DataFrame()  # 최종 합치기\n",
    "        \n",
    "\n",
    "        # ctgan으로 연속형 생성 부분\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(data=midlle_small_X)\n",
    "        \n",
    "        synthesizer = SingleTablePreset(metadata, name='FAST_ML')\n",
    "        synthesizer.fit(data=midlle_small_X)\n",
    "                \n",
    "            \n",
    "        # large class의 데이터 사이즈 만큼 데이터 생성\n",
    "        synthetic_data = synthesizer.sample(num_rows=len(large_X)) # 새로운 데이터를 생성하는 비용 vs 데이터가 적합한지 판단하는 비용 \n",
    "                \n",
    "\n",
    "        # 합성된 개수 / 원래 클래스 개수 <= ratio 만족시 그만 생성하는 것으로\n",
    "        i = 0 \n",
    "        while len(synthetic_sample) / len(large_X) >= self.ratio:\n",
    "            \n",
    "            z = synthetic_data.loc[i]\n",
    "\n",
    "            # 생성된 small class 데이터가 small, large class 중 small에 가까운지, small class의 지름을 넘지는 않는지\n",
    "            if (\n",
    "                np.linalg.norm(z - center_small_X) < np.linalg.norm(z - center_large_X)\n",
    "                and np.linalg.norm(z - center_small_X) < radius_small_X\n",
    "            ):\n",
    "                synthetic_sample.append(z)  \n",
    "                \n",
    "            i+=1\n",
    "            \n",
    "            # 생성된 샘플을 다 검정해도 생성 비율을 만족하지 못할 경우\n",
    "            if i+1 == len(large_X) and len(synthetic_sample) / len(large_X) < self.ratio:\n",
    "                i=0\n",
    "                synthetic_data = synthesizer.sample(num_rows=len(large_X))\n",
    "                \n",
    "        return synthetic_sample.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    def generate_synthetic(\n",
    "        self, small_X: pd.DataFrame, large_X: pd.DataFrame, small_Y: pd.DataFrame, large_Y: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 생성\n",
    "        \n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "        Returns:\n",
    "            생성된 데이터셋 + 기존 데이터셋을 합쳐 통합 데이터셋을 리턴\n",
    "        \"\"\"\n",
    "        # 연속형 변수만 가져오는 부분\n",
    "        continue_small_X = small_X[self.index]\n",
    "        continue_large_X = large_X[self.index]\n",
    "\n",
    "        # 범주형 변수만 가져오는 부분\n",
    "        categorical_small_X = small_X[list(set(small_X.columns) - set(self.index))]\n",
    "        categorical_large_X = large_X[list(set(small_X.columns) - set(self.index))]\n",
    "\n",
    "        # 상위 n% 필터링 부분\n",
    "        midlle_small_X = self.extract_middle_percent(\n",
    "            continue_small_X, 25, 75\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "\n",
    "        midlle_large_X = self.extract_middle_percent(\n",
    "            continue_large_X, 15, 85\n",
    "        )  ##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\n",
    "\n",
    "        # 연속형 데이터 생성 및 데이터 적합 판단\n",
    "\n",
    "        suitable_generated_small_X = self.suitable_judge(midlle_small_X, small_X, large_X)\n",
    "\n",
    "        # 코사인 유사도 기반으로 가장 가까운 기존 변수의 범주형 변수 값 가져오기\n",
    "\n",
    "        synthetic_small_X = self.find_categorical(\n",
    "            suitable_generated_small_X, categorical_small_X, small_X ,self.index\n",
    "        )\n",
    "\n",
    "        # small class와 large class 합치기\n",
    "\n",
    "        origin_small_x = pd.concat(\n",
    "            [midlle_small_X, categorical_small_X.loc[midlle_small_X.index]], axis=1\n",
    "        )\n",
    "\n",
    "        small_total_x = pd.concat([synthetic_small_X, origin_small_x], axis=0)\n",
    "\n",
    "        small_total_x[\"target\"] = small_Y[0]\n",
    "\n",
    "        origin_large_x = pd.concat(\n",
    "            [midlle_large_X, categorical_large_X.loc[midlle_large_X.index]], axis=1\n",
    "        )\n",
    "\n",
    "        origin_large_x[\"target\"] = small_Y[0]\n",
    "        total = pd.concat([small_total_x, origin_large_x], axis=0)\n",
    "        return total.drop(columns=[\"target\"]), total[\"target\"]\n",
    "    \n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        small_X: pd.DataFrame,\n",
    "        small_Y: pd.DataFrame,\n",
    "        large_X: pd.DataFrame,\n",
    "        large_Y: pd.DataFrame        \n",
    "    ):\n",
    "        \"\"\"\n",
    "        데이터를 학습 시키는 함수\n",
    "        Args:\n",
    "            small_X (pd.DataFrame): small class의 x\n",
    "            small_Y (pd.DataFrame): small class의 y\n",
    "            large_X (pd.DataFrame): large class의 x\n",
    "            large_Y (pd.DataFrame): large class의 y\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: synthetic X, y\n",
    "        \n",
    "        \"\"\"\n",
    "        # 합성+ 기존 data set 생성\n",
    "        synthetic_X, synthetic_Y = self.generate_synthetic(\n",
    "            small_X, large_X,small_Y, large_Y\n",
    "        )\n",
    "        return synthetic_X, synthetic_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN = FiGen(0.3, ['guest_email','room_type', 'checkin_date','checkout_date','billing_address','credit_card_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'webermelissa@neal.com'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ratio \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m index \u001b[39m=\u001b[39m [\u001b[39m2\u001b[39m,\u001b[39m5\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m GEN\u001b[39m.\u001b[39;49mfit(small_X, small_Y, large_X, large_Y)\n",
      "\u001b[1;32m/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb 셀 6\u001b[0m in \u001b[0;36mFiGen.fit\u001b[0;34m(self, small_X, small_Y, large_X, large_Y)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=228'>229</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m \u001b[39m데이터를 학습 시키는 함수\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=237'>238</a>\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=239'>240</a>\u001b[0m \u001b[39m# 합성+ 기존 data set 생성\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=240'>241</a>\u001b[0m synthetic_X, synthetic_Y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_synthetic(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=241'>242</a>\u001b[0m     small_X, large_X,small_Y, large_Y\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=242'>243</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m \u001b[39mreturn\u001b[39;00m synthetic_X, synthetic_Y\n",
      "\u001b[1;32m/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb 셀 6\u001b[0m in \u001b[0;36mFiGen.generate_synthetic\u001b[0;34m(self, small_X, large_X, small_Y, large_Y)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m categorical_large_X \u001b[39m=\u001b[39m large_X[\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(small_X\u001b[39m.\u001b[39mcolumns) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex))]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m \u001b[39m# 상위 n% 필터링 부분\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m midlle_small_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_middle_percent(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m     continue_small_X, \u001b[39m25\u001b[39;49m, \u001b[39m75\u001b[39;49m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m )  \u001b[39m##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m midlle_large_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_middle_percent(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m     continue_large_X, \u001b[39m15\u001b[39m, \u001b[39m85\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m )  \u001b[39m##TODO: 추후에 하이퍼 파라미터로 뺄 수 있음\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m \u001b[39m# 연속형 데이터 생성 및 데이터 적합 판단\u001b[39;00m\n",
      "\u001b[1;32m/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb 셀 6\u001b[0m in \u001b[0;36mFiGen.extract_middle_percent\u001b[0;34m(self, data, start, last)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m데이터의 분포 중 중간 부분을 추출 \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m    데이터의 분포 중 중간 부분을 추출하여 리턴\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m data_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m kde \u001b[39m=\u001b[39m KernelDensity(kernel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgaussian\u001b[39m\u001b[39m\"\u001b[39m, bandwidth\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     data_scaled\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m )  \u001b[39m##TODO: 계산이 안터지도록 하기, gmm으로 변경\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jangsehwan/Financial-Imbalanced-Data-With-Deep-Density-Hybrid-Sampling/scripts/f_ddhs_v0.02.00.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m log_prob \u001b[39m=\u001b[39m kde\u001b[39m.\u001b[39mscore_samples(data_scaled)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 806\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:841\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \n\u001b[1;32m    811\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    840\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 841\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    842\u001b[0m     X,\n\u001b[1;32m    843\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    844\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    845\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    846\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    847\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    849\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'webermelissa@neal.com'"
     ]
    }
   ],
   "source": [
    "# y = has_rewards\n",
    "small_X = real_data[real_data['has_rewards'] == True]\n",
    "small_Y = real_data[real_data['has_rewards'] == True].iloc[:, [1]]\n",
    "large_X = real_data[real_data['has_rewards'] == False]\n",
    "large_Y = real_data[real_data['has_rewards'] == False].iloc[:, [1]]\n",
    "ratio = 0.3\n",
    "index = [2,5]\n",
    "GEN.fit(small_X, small_Y, large_X, large_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_index = ['guest_email','room_type', 'checkin_date','checkout_date','billing_address','credit_card_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['has_rewards', 'room_rate', 'amenities_fee']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(real_data.columns) - set(con_index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
